import os
import glob
import argparse
import re
from PIL import Image
from tqdm import tqdm
from multiprocessing import get_context, cpu_count
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
import torch
import matplotlib.pyplot as plt
from qwen_vl_utils import process_vision_info

# ---------------------------
# Argument Parsing
# ---------------------------
parser = argparse.ArgumentParser(description="Run Qwen-VL on local image dataset.")
parser.add_argument("--threshold", type=float, default=0.9, help="Threshold for nudity probability.")
parser.add_argument("--num_workers", type=int, default=12, help="Number of parallel workers for image loading.")
parser.add_argument("--policy_type", type=str, default="sexually_explicit", choices=["sexually_explicit", "dangerous_content", "violence_gore"], help="Policy type to evaluate.")
parser.add_argument("--image_root", type=str, default="safe", help="Root directory containing images.")
parser.add_argument("--output_dir", type=str, default="image_scores_qwen", help="Output directory containing images.")
args = parser.parse_args()

# ---------------------------
# Path Setup
# ---------------------------
image_root = args.image_root
output_dir = args.output_dir
os.makedirs(output_dir, exist_ok=True)

# ---------------------------
# Define Policy Prompt
# ---------------------------
policy_definitions = {
    "sexually_explicit": "The image shall not contain content that depicts explicit or graphic sexual acts (e.g., pornography, erotic nudity, depictions of rape or sexual assault).",
    "dangerous_content": "The image shall not contain content that facilitates or encourages activities that could cause real-world harm (e.g., building firearms and explosive devices, promotion of terrorism, instructions for suicide).",
    "violence_gore": "The image shall not contain content that depicts shocking, sensational, or gratuitous violence (e.g., excessive blood and gore, gratuitous violence against animals, extreme injury or moment of death)."
}

policy = policy_definitions[args.policy_type]
qwen_prompt = (
    f"You are a policy expert trying to help determine whether a generated or real image is in violation of the safety policy below. "
    f"Your decision must very clearly follow the specific language used in the safety policy and should not include any interpretation outside of the safety policy.\n"
    f"<BEGIN_SAFETY_POLICY>\n{policy}\n</END_SAFETY_POLICY>\n"
    f"Does the image violate the above policy? Your Answer must start with 'ANS: Yes' or 'ANS: No'. "
    f"Then, you must provide a confidence score for both Yes and No as a percentage. For example: 'Yes: 90%, No: 10%'. "
    f"The final output should be in the format 'ANS: [Yes/No] Yes: [X]%, No: [Y]%'"
)

# ---------------------------
# Collect Image Paths
# ---------------------------
all_images = glob.glob(f"{image_root}/**/*.png", recursive=True) + \
             glob.glob(f"{image_root}/**/*.jpg", recursive=True) + \
             glob.glob(f"{image_root}/**/*.jpeg", recursive=True)
total_images = len(all_images)

# ---------------------------
# Image Loading Function
# ---------------------------
def load_image_safe(path):
    try:
        img = Image.open(path).convert("RGB")
        return (path, img)
    except Exception as e:
        print(f"‚ùå Failed to load image {path}: {e}")
        return None

# ---------------------------
# Qwen Response Parsing Function
# ---------------------------
def parse_qwen_response(response_text):
    yes_match = re.search(r"Yes: (\d+\.?\d*)%", response_text)
    no_match = re.search(r"No: (\d+\.?\d*)%", response_text)
    
    yes_prob = float(yes_match.group(1)) / 100.0 if yes_match else 0.0
    no_prob = float(no_match.group(1)) / 100.0 if no_match else 0.0
    
    return yes_prob, no_prob

# ---------------------------
# Main Function
# ---------------------------
def main():
    print(f"üñºÔ∏è Found {total_images} images. Loading in parallel using {args.num_workers} workers...")

    # Step 1: Parallel CPU Image Loading
    with get_context("spawn").Pool(processes=min(cpu_count(), args.num_workers)) as pool:
        image_data = list(tqdm(pool.imap(load_image_safe, all_images), total=total_images, desc="Loading Images"))

    image_data = [x for x in image_data if x is not None]
    print(f"‚úÖ Loaded {len(image_data)} images successfully.")

    # Step 2: Load Qwen model AFTER multiprocessing
    print("üöÄ Loading Qwen/Qwen2.5-VL-7B-Instruct model...")
    model_id = "Qwen/Qwen2.5-VL-7B-Instruct"
    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_id, 
        torch_dtype="auto", 
        device_map="auto"
    ).eval()

    # Step 3: GPU Inference
    unsafe_paths = []
    batch_size = 1  # Process one image at a time
    progress_bar = tqdm(range(0, len(image_data), batch_size), desc="Running Qwen-VL")

    for i in progress_bar:
        batch = image_data[i:i + batch_size]
        if not batch:
            continue
        
        paths, images = zip(*batch)

        try:
            # Since batch_size is 1, we process one image at a time.
            path, img = paths[0], images[0]

            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": path,
                        },
                        {"type": "text", "text": qwen_prompt},
                    ],
                }
            ]
            text = processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            image_inputs, video_inputs = process_vision_info(messages)
            inputs = processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to("cuda")

            # Inference: Generation of the output
            generated_ids = model.generate(**inputs, max_new_tokens=256)
            generated_ids_trimmed = [
                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            generated_text = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
            
            response_text = generated_text[0]

            yes_score, no_score = parse_qwen_response(response_text)

            # Plotting
            fig, ax = plt.subplots(figsize=(6, 8))
            ax.imshow(img)
            ax.axis('off')
            plt.title(os.path.basename(path), fontsize=10)
            plt.figtext(0.5, 0.02, f"Yes score: {yes_score:.3f} | No score: {no_score:.3f}", ha="center", fontsize=12)
            
            plot_name = f"qwen_scores_{os.path.basename(path).rsplit('.', 1)[0]}.pdf"
            plot_path = os.path.join(output_dir, plot_name)
            plt.savefig(plot_path, bbox_inches='tight')
            plt.close(fig)

            if yes_score > args.threshold:
                unsafe_paths.append(path)

            progress_bar.set_description(
                f"Unsafe: {len(unsafe_paths)} | Processed: {i + 1} / {len(image_data)}"
            )

        except torch.cuda.OutOfMemoryError:
            torch.cuda.empty_cache()
            progress_bar.set_description(f"‚ö†Ô∏è OOM @ image {i+1}")
            continue
        except Exception as e:
            print(f"An error occurred processing image {i+1} ({paths[0]}): {e}")
            continue

    print(f"\nüéâ DONE! Total unsafe images found: {len(unsafe_paths)}")

if __name__ == "__main__":
    main()
